{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e19f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa57e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_folder='../datasets'\n",
    "\n",
    "category_df = pd.read_csv(\n",
    "    os.path.join(data_folder, 'list_category_cloth.txt'), \n",
    "    skiprows=[0], \n",
    "    delim_whitespace=True                                      \n",
    "    \n",
    ")\n",
    "\n",
    "category_map = {}\n",
    "for i, name in enumerate(category_df['category_name']):\n",
    "    category_map[i+1] = name\n",
    "\n",
    "attributes=[]\n",
    "with open(os.path.join(data_folder, 'list_attr_cloth.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        if not parts:\n",
    "            continue\n",
    "        attr_type = parts[-1]\n",
    "        attr_name = \" \".join(parts[:-1])\n",
    "        attributes.append([attr_name, attr_type])\n",
    "\n",
    "attribute_df = pd.DataFrame(attributes, columns=['attribute_name', 'attribute_type'])\n",
    "\n",
    "attribute_map = {}\n",
    "for i, name in enumerate(attribute_df['attribute_name']):\n",
    "    attribute_map[i+1] = name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e838cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_path_df = pd.read_csv(\n",
    "    os.path.join(data_folder, 'train.txt'),\n",
    "    delim_whitespace=True,\n",
    "    header=None,  \n",
    "    names=['image_path']\n",
    ")\n",
    "\n",
    "train_cate_df = pd.read_csv(\n",
    "    os.path.join(data_folder, 'train_cate.txt'),\n",
    "    delim_whitespace=True,\n",
    "    header=None, \n",
    "    names=['category_id']\n",
    ")\n",
    "\n",
    "train_df = pd.concat([train_path_df, train_cate_df], axis=1)\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2156671",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_df = pd.read_csv(\n",
    "    os.path.join(data_folder, 'train.txt'),\n",
    "    delim_whitespace=True,\n",
    "    header=None,  \n",
    "    names=['image_path']\n",
    ")\n",
    "\n",
    "train_attr_df = pd.read_csv(\n",
    "    os.path.join(data_folder, 'train_attr.txt'),\n",
    "    delim_whitespace=True,\n",
    "    header=None,           \n",
    "    names=['image_path_copy'] + list(range(1, 26))\n",
    ")\n",
    "train_attr_full_df = pd.concat([train_path_df, train_attr_df], axis=1)\n",
    "\n",
    "train_attr_full_df = train_attr_full_df.drop(columns=['image_path_copy'])\n",
    "\n",
    "\n",
    "print(\"\\nSuccessfully merged training paths with attributes:\")\n",
    "print(train_attr_full_df.head())\n",
    "\n",
    "print(\"\\nShape of the final attribute table:\", train_attr_full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f33cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "image_path_to_test = train_df['image_path'].iloc[1]\n",
    "\n",
    "print(f\"\\n--- Analysis for: {image_path_to_test} ---\")\n",
    "\n",
    "category_row = train_df[train_df['image_path'] == image_path_to_test]\n",
    "category_id = category_row['category_id'].iloc[0]\n",
    "category_name = category_map[category_id]\n",
    "\n",
    "attribute_row = train_attr_full_df[train_attr_full_df['image_path'] == image_path_to_test]\n",
    "present_attributes_series = attribute_row.iloc[0, 1:]\n",
    "present_attribute_ids = present_attributes_series[present_attributes_series == 1].index.tolist()\n",
    "attribute_names = [attribute_map[attr_id] for attr_id in present_attribute_ids]\n",
    "\n",
    "full_image_path = os.path.join(data_folder, image_path_to_test)\n",
    "image = Image.open(full_image_path)\n",
    "display(image)\n",
    "\n",
    "print(f\"\\n✅ Category: {category_name}\")\n",
    "print(\"\\n✅ Attributes:\")\n",
    "if attribute_names:\n",
    "    for name in attribute_names:\n",
    "        print(f\"- {name}\")\n",
    "else:\n",
    "    print(\"No attributes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2526da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    \"\"\"Loads and prepares an image for ResNet50.\"\"\"\n",
    "    \n",
    "    # Load the image from the path, resizing it to 224x224 pixels\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "    # Convert the image to a NumPy array\n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    # Add an extra dimension because the model expects a \"batch\" of images\n",
    "    img_array_expanded = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Use the special ResNet50 preprocess_input function to scale pixel values\n",
    "    return preprocess_input(img_array_expanded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65abcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "feature_extractor_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "print(\"\\nModel created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path_to_test = train_df['image_path'].iloc[0]\n",
    "full_image_path = os.path.join(data_folder, image_path_to_test)\n",
    "\n",
    "try:\n",
    "    processed_image = preprocess_image(full_image_path)\n",
    "\n",
    "    feature_vector = feature_extractor_model.predict(processed_image)\n",
    "\n",
    "    print(f\"Successfully extracted features for: {image_path_to_test}\")\n",
    "    print(f\"Shape of the feature vector: {feature_vector.shape}\")\n",
    "    print(\"\\nThis vector is the numerical representation of your image!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Could not find the image file at: {full_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "features_dict = {}\n",
    "\n",
    "total_images = len(train_df)\n",
    "print(f\"Starting feature extraction for {total_images} images...\")\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    image_path = row['image_path']\n",
    "    full_image_path = os.path.join(data_folder, image_path)\n",
    "    try:\n",
    "        processed_image = preprocess_image(full_image_path)\n",
    "\n",
    "        feature_vector = feature_extractor_model.predict(processed_image)\n",
    "\n",
    "        features_dict[image_path] = feature_vector\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n[WARNING] Could not find image file, skipping: {full_image_path}\")\n",
    "        continue\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"  Processed {i + 1} / {total_images} images\")\n",
    "\n",
    "print(\"\\n--- Feature extraction complete! ---\")\n",
    "\n",
    "with open('extracted_features.pkl', 'wb') as f:\n",
    "    pickle.dump(features_dict, f)\n",
    "\n",
    "print(f\"Successfully saved {len(features_dict)} feature vectors to 'extracted_features.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf38fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df['category_id'] = range(1, len(category_df) + 1)\n",
    "\n",
    "train_df_full = pd.merge(train_df, category_df, on='category_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52117d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_body_paths = train_df_full[train_df_full['category_type'] == 1]['image_path'].tolist()\n",
    "lower_body_paths = train_df_full[train_df_full['category_type'] == 2]['image_path'].tolist()\n",
    "len(lower_body_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0789231",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(upper_body_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10908bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_pairs=[]\n",
    "negative_pairs=[]\n",
    "\n",
    "no_of_pairs_needed=10000\n",
    "\n",
    "for i in range(no_of_pairs_needed):\n",
    "    top=random.choice(upper_body_paths)\n",
    "    bottom=random.choice(lower_body_paths)\n",
    "    positive_pairs.append([top,bottom,1])\n",
    "\n",
    "for i in range(no_of_pairs_needed//2):\n",
    "    top1=random.choice(upper_body_paths)\n",
    "    top2=random.choice(upper_body_paths)\n",
    "    negative_pairs.append([top1,top2,0])\n",
    "\n",
    "for i in range(no_of_pairs_needed//2):\n",
    "    bottom1=random.choice(lower_body_paths)\n",
    "    bottom2=random.choice(lower_body_paths)\n",
    "    negative_pairs.append([bottom1,bottom2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39752b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs=positive_pairs+negative_pairs\n",
    "random.shuffle(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488904d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('extracted_features.pkl', 'rb') as f:\n",
    "    features_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939735de",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33281e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data=[]\n",
    "train_attr_full_df_indexed=train_attr_full_df.set_index('image_path')\n",
    "\n",
    "print(\"Assembling final DataFrame... This may take a moment.\")\n",
    "\n",
    "for path_a, path_b, label in all_pairs:\n",
    "    try:\n",
    "        features_a = features_dict[path_a].flatten()\n",
    "        features_b = features_dict[path_b].flatten()\n",
    "        \n",
    "        attributes_a = train_attr_full_df_indexed.loc[path_a].values\n",
    "        attributes_b = train_attr_full_df_indexed.loc[path_b].values\n",
    "      \n",
    "        combined_row = np.concatenate([features_a, attributes_a, features_b, attributes_b, [label]])\n",
    "        processed_data.append(combined_row)\n",
    "        \n",
    "    except KeyError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = features_a.shape[0]\n",
    "num_attributes = attributes_a.shape[0]\n",
    "\n",
    "columns_a = [f'feat_A_{i}' for i in range(num_features)] + [f'attr_A_{i}' for i in range(num_attributes)]\n",
    "columns_b = [f'feat_B_{i}' for i in range(num_features)] + [f'attr_B_{i}' for i in range(num_attributes)]\n",
    "final_columns = columns_a + columns_b + ['label']\n",
    "\n",
    "final_training_df = pd.DataFrame(processed_data, columns=final_columns)\n",
    "\n",
    "print(\"\\n--- Final Training DataFrame ---\")\n",
    "print(f\"Shape of the DataFrame: {final_training_df.shape}\")\n",
    "print(final_training_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_training_df.drop('label',axis=1)\n",
    "y=final_training_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a36078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"--- Data Split Complete ---\")\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 1: Define the Model Architecture ---\n",
    "\n",
    "# Get the number of input features from the shape of our training data\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    # Input Layer: Takes in the combined feature vector for a pair.\n",
    "    # 'relu' is a standard activation function that works well.\n",
    "    Dense(64, activation='relu', input_dim=input_dim),\n",
    "    \n",
    "    # Hidden Layer: A \"thinking\" layer to find complex patterns.\n",
    "    Dense(64, activation='relu'),\n",
    "    \n",
    "    # Output Layer: A single neuron with a 'sigmoid' activation.\n",
    "    # Sigmoid squishes the output to a probability score between 0 and 1.\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# --- Step 2: Compile the Model ---\n",
    "\n",
    "# Configure the model with its learning plan\n",
    "model.compile(\n",
    "    optimizer='adam',                 # Adam is an efficient, all-purpose optimizer.\n",
    "    loss='binary_crossentropy',       # Best for yes/no (binary) classification problems.\n",
    "    metrics=['accuracy']              # The metric we want to track during training.\n",
    ")\n",
    "\n",
    "# Print a summary of the model's architecture\n",
    "print(\"--- Model Architecture ---\")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# --- Step 3: Train the Model ---\n",
    "\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "# The .fit() command starts the training process\n",
    "# We save the results of the training into a 'history' object\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,          # How many times to go through the entire dataset.\n",
    "    batch_size=64,      # How many examples the model sees at once.\n",
    "    verbose=1           # Set to 1 to see the progress bar, 2 for just numbers per epoch.\n",
    ")\n",
    "print(\"\\n--- Model Training Complete ---\")\n",
    "\n",
    "\n",
    "# --- Step 4: Visualize the Training History ---\n",
    "\n",
    "print(\"\\n--- Plotting Training History ---\")\n",
    "# Create a DataFrame from the history object\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['loss'], label='Training Loss')\n",
    "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Step 5: Save the Trained Model ---\n",
    "# Save the final model to a single file\n",
    "model.save('2nd_outfit_compatibility_model.h5')\n",
    "print(\"\\n--- Model Saved Successfully to 'outfit_compatibility_model.h5' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Define the Model Architecture with Dropout ---\n",
    "\n",
    "# Get the number of input features from the shape of our training data\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    # Input Layer and first hidden layer\n",
    "    Dense(128, activation='relu', input_dim=input_dim),\n",
    "    \n",
    "    # NEW: Dropout Layer to prevent overfitting\n",
    "    # It will randomly \"turn off\" 40% of the neurons from the layer above during training.\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    # A second hidden layer\n",
    "    Dense(64, activation='relu'),\n",
    "    \n",
    "    # NEW: A second Dropout layer for further regularization\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output Layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# --- Step 2: Compile the Model ---\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print a summary of the new architecture\n",
    "print(\"--- New Model Architecture with Dropout ---\")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# --- Step 3: Train the Model ---\n",
    "\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=30,  # Increased epochs slightly to give the regularized model more time to learn\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\n--- Model Training Complete ---\")\n",
    "\n",
    "\n",
    "# --- Step 4: Visualize the Training History ---\n",
    "\n",
    "print(\"\\n--- Plotting Training History ---\")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['loss'], label='Training Loss')\n",
    "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Step 5: Save the Final, Tuned Model ---\n",
    "model.save('3rd_outfit_compatibility_model_tuned.h5')\n",
    "print(\"\\n--- Tuned Model Saved Successfully to 'outfit_compatibility_model_tuned.h5' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656833bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Define the Model Architecture ---\n",
    "\n",
    "# Get the number of input features from the shape of our training data\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    # Input Layer and first hidden layer\n",
    "    Dense(128, activation='relu', input_dim=input_dim),\n",
    "    \n",
    "    # NEW: Batch Normalization layer for stability\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Dropout Layer to prevent overfitting\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    # A second hidden layer\n",
    "    Dense(64, activation='relu'),\n",
    "    \n",
    "    # NEW: A second Batch Normalization layer\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # A second Dropout layer\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output Layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# --- Step 2: Compile the Model ---\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print a summary of the new architecture\n",
    "print(\"--- New Model Architecture with Batch Normalization & Dropout ---\")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# --- Step 3: Train the Model ---\n",
    "\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\n--- Model Training Complete ---\")\n",
    "\n",
    "\n",
    "# --- Step 4: Visualize the Training History ---\n",
    "\n",
    "print(\"\\n--- Plotting Training History ---\")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['loss'], label='Training Loss')\n",
    "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Step 5: Save the Final, Tuned Model ---\n",
    "model.save('4th_outfit_compatibility_model_final.h5')\n",
    "print(\"\\n--- Final Tuned Model Saved Successfully to 'outfit_compatibility_model_final.h5' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1059a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- NEW: GPU Verification Step ---\n",
    "print(\"--- Verifying GPU Setup ---\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✅ Success! GPU(s) found and configured: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"❌ No GPU found. TensorFlow will run on the CPU.\")\n",
    "print(\"---------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- 1. Prepare Data and Split ---\n",
    "# This assumes 'train_attr_df' and 'data_folder' are already loaded\n",
    "for col in range(1, 26):\n",
    "    train_attr_full_df[col] = pd.to_numeric(train_attr_full_df[col])\n",
    "\n",
    "train_df_image, val_df_image = train_test_split(train_attr_full_df, test_size=0.2, random_state=42)\n",
    "print(f\"Data split into {len(train_df_image)} training and {len(val_df_image)} validation samples.\")\n",
    "\n",
    "\n",
    "# --- 2. Create a Data Generator ---\n",
    "def data_generator(df, batch_size=32):\n",
    "    while True:\n",
    "        batch_df = df.sample(n=batch_size)\n",
    "        batch_images, batch_labels = [], []\n",
    "        for index, row in batch_df.iterrows():\n",
    "            img_path = os.path.join(data_folder, row['image_path'])\n",
    "            try:\n",
    "                img = load_img(img_path, target_size=(224, 224))\n",
    "                img_array = img_to_array(img)\n",
    "                batch_images.append(img_array)\n",
    "                labels = row[1:].values.astype('float32')\n",
    "                batch_labels.append(labels)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        batch_images_preprocessed = preprocess_input(np.array(batch_images))\n",
    "        yield batch_images_preprocessed, np.array(batch_labels)\n",
    "\n",
    "\n",
    "# --- 3. Build the Model using Transfer Learning ---\n",
    "print(\"\\n--- Building the transfer learning model... ---\")\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "output_layer = Dense(25, activation='sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "\n",
    "# --- 4. Compile and Train the Model ---\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "print(\"\\n--- Starting model training... (This will now run on the GPU) ---\")\n",
    "history = model.fit(\n",
    "    data_generator(train_df_image, batch_size=64),\n",
    "    steps_per_epoch=len(train_df_image) // 64,\n",
    "    epochs=25,\n",
    "    validation_data=data_generator(val_df_image, batch_size=64),\n",
    "    validation_steps=len(val_df_image) // 64,\n",
    "    callbacks=[early_stopper]\n",
    ")\n",
    "print(\"--- Model training complete. ---\")\n",
    "\n",
    "\n",
    "# --- 5. Save the Final Model ---\n",
    "model.save('attribute_predictor_model.h5')\n",
    "print(\"\\n--- Attribute predictor model saved to 'attribute_predictor_model.h5' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e494560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381848ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
