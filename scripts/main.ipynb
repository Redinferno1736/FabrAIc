{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04511040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67491001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_folder='../datasets'\n",
    "\n",
    "category_df = pd.read_csv(\n",
    "    os.path.join(data_folder, 'list_category_cloth.txt'), \n",
    "    skiprows=[0], \n",
    "    delim_whitespace=True                                      \n",
    "    \n",
    ")\n",
    "\n",
    "category_map = {}\n",
    "for i, name in enumerate(category_df['category_name']):\n",
    "    category_map[i+1] = name\n",
    "\n",
    "attributes=[]\n",
    "with open(os.path.join(data_folder, 'list_attr_cloth.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        if not parts:\n",
    "            continue\n",
    "        attr_type = parts[-1]\n",
    "        attr_name = \" \".join(parts[:-1])\n",
    "        attributes.append([attr_name, attr_type])\n",
    "\n",
    "attribute_df = pd.DataFrame(attributes, columns=['attribute_name', 'attribute_type'])\n",
    "\n",
    "attribute_map = {}\n",
    "for i, name in enumerate(attribute_df['attribute_name']):\n",
    "    attribute_map[i+1] = name\n",
    "\n",
    "attribute_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c99e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "eval_file = os.path.join(data_folder, 'list_eval_partition.txt')\n",
    "cate_file = os.path.join(data_folder, 'list_category_img.txt')\n",
    "\n",
    "if os.path.exists(eval_file) and os.path.exists(cate_file):\n",
    "    df_partition = pd.read_csv(eval_file, sep='\\s+', skiprows=[0, 1], header=None, names=['image_path', 'split'])\n",
    "\n",
    "    df_category = pd.read_csv(cate_file, sep='\\s+', skiprows=[0, 1], header=None, names=['image_path', 'category_id'])\n",
    "\n",
    "    train_df = pd.merge(df_partition, df_category, on='image_path')\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len(train_df)} images.\")\n",
    "    print(\"Split breakdown:\")\n",
    "    print(train_df['split'].value_counts())\n",
    "    \n",
    "    print(\"\\nSample Data:\")\n",
    "    print(train_df.head())\n",
    "\n",
    "else:\n",
    "    print(f\"ERROR: Could not find 'list_eval_partition.txt' or 'list_category_img.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb408486",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_df = pd.read_csv(\n",
    "    os.path.join(data_folder, 'train.txt'),\n",
    "    delim_whitespace=True,\n",
    "    header=None,  \n",
    "    names=['image_path']\n",
    ")\n",
    "\n",
    "train_attr_df = pd.read_csv(\n",
    "    os.path.join(data_folder, 'train_attr.txt'),\n",
    "    delim_whitespace=True,\n",
    "    header=None,           \n",
    "    names=['image_path_copy'] + list(range(1, 26))\n",
    ")\n",
    "train_attr_full_df = pd.concat([train_path_df, train_attr_df], axis=1)\n",
    "\n",
    "train_attr_full_df = train_attr_full_df.drop(columns=['image_path_copy'])\n",
    "\n",
    "\n",
    "print(\"\\nSuccessfully merged training paths with attributes:\")\n",
    "print(train_attr_full_df.head())\n",
    "\n",
    "print(\"\\nShape of the final attribute table:\", train_attr_full_df.shape)\n",
    "\n",
    "final_train_df = pd.merge(train_attr_full_df, df_category, on='image_path', how='inner')\n",
    "\n",
    "print(\"Final Training Data Ready.\")\n",
    "print(len(final_train_df))\n",
    "print( final_train_df.columns.tolist())\n",
    "final_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42463bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = self.df['image_path'].values\n",
    "        self.categories = self.df['category_id'].values - 1\n",
    "        self.attributes = self.df.loc[:, 1:25].values.astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except (OSError, FileNotFoundError):\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        category_label = torch.tensor(self.categories[idx], dtype=torch.long)\n",
    "        attribute_labels = torch.tensor(self.attributes[idx], dtype=torch.float32)\n",
    "        return image, category_label, attribute_labels\n",
    "\n",
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = FashionDataset(\n",
    "    dataframe=final_train_df, \n",
    "    root_dir=data_folder, \n",
    "    transform=transform_pipeline\n",
    ")\n",
    "print(f\"Dataset created with {len(dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cat, attr = dataset[0]\n",
    "print(\"Image shape:\", img.shape)      \n",
    "print(\"Category:\", cat)               \n",
    "\n",
    "print(\"Attributes shape:\", attr.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bac068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "class MultiTaskResNet(nn.Module):\n",
    "    def __init__(self, num_categories=50, num_attributes=25):\n",
    "        super(MultiTaskResNet, self).__init__()\n",
    "        self.backbone = models.resnet50(weights='DEFAULT')\n",
    "        n_inputs = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.category_head = nn.Linear(n_inputs, num_categories)\n",
    "        self.attribute_head = nn.Linear(n_inputs, num_attributes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        cat_output = self.category_head(features)\n",
    "        attr_output = self.attribute_head(features)\n",
    "        return cat_output, attr_output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Training on Device: {device}\")\n",
    "\n",
    "model = MultiTaskResNet(num_categories=50, num_attributes=25)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion_cat = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_attr = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "num_epochs = 3  \n",
    "\n",
    "print(f\"\\n Starting Training for {num_epochs} Epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, cat_labels, attr_labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        cat_labels = cat_labels.to(device)\n",
    "        attr_labels = attr_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cat_preds, attr_preds = model(images)\n",
    "        \n",
    "        loss_cat = criterion_cat(cat_preds, cat_labels)\n",
    "        loss_attr = criterion_attr(attr_preds, attr_labels)\n",
    "        \n",
    "        total_loss = loss_cat + loss_attr \n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  > Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"üèÅ Epoch {epoch+1} Finished. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n Training Complete!\")\n",
    "\n",
    "torch.save(model.state_dict(), \"fashion_model_v1.pth\")\n",
    "print(\" Model saved to 'fashion_model_v1.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "full_catalog_df = pd.read_csv(\n",
    "    os.path.join(data_folder, 'list_eval_partition.txt'),\n",
    "    sep='\\s+', \n",
    "    skiprows=[0, 1], \n",
    "    header=None, \n",
    "    names=['image_path', 'split']\n",
    ")\n",
    "\n",
    "print(f\"Total images to index: {len(full_catalog_df)}\")\n",
    "\n",
    "class FashionInferenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = self.df['image_path'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, self.image_paths[idx]\n",
    "\n",
    "inference_dataset = FashionInferenceDataset(\n",
    "    full_catalog_df, \n",
    "    data_folder, \n",
    "    transform=transform_pipeline\n",
    ")\n",
    "\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=64, shuffle=False)\n",
    "print(\"‚úÖ Inference Loader Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05537bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class MultiTaskResNet(nn.Module):\n",
    "    def __init__(self, num_categories=50, num_attributes=25):\n",
    "        super(MultiTaskResNet, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=None) \n",
    "        n_inputs = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.category_head = nn.Linear(n_inputs, num_categories)\n",
    "        self.attribute_head = nn.Linear(n_inputs, num_attributes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        cat_output = self.category_head(features)\n",
    "        attr_output = self.attribute_head(features)\n",
    "        return cat_output, attr_output\n",
    "\n",
    "MODEL_PATH = \"fashion_model_v1.pth\"\n",
    "OUTPUT_INDEX_PATH = \"fashion_index_v1.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Device set to: {device}\")\n",
    "\n",
    "model = MultiTaskResNet(num_categories=50, num_attributes=25)\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"üìÇ Loading weights from '{MODEL_PATH}'...\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\" Model loaded and ready for indexing.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\" Error: {MODEL_PATH} not found. Did you run the training cell?\")\n",
    "\n",
    "print(\"Starting Feature Extraction...\")\n",
    "all_features = []\n",
    "all_paths = []\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(inference_loader, desc=\"Indexing\"):\n",
    "        images = images.to(device)\n",
    "        features = model.backbone(images)\n",
    "        all_features.append(features.cpu().numpy())\n",
    "        all_paths.extend(paths)\n",
    "\n",
    "print(\"\\n Stacking features...\")\n",
    "final_embeddings = np.concatenate(all_features, axis=0)\n",
    "\n",
    "index_data = {\n",
    "    \"paths\": all_paths,\n",
    "    \"embeddings\": final_embeddings\n",
    "}\n",
    "\n",
    "with open(OUTPUT_INDEX_PATH, \"wb\") as f:\n",
    "    pickle.dump(index_data, f)\n",
    "\n",
    "print(f\"SUCCESS! Index saved to '{OUTPUT_INDEX_PATH}'\")\n",
    "print(f\"Matrix Shape: {final_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"fashion_model_v1.pth\"\n",
    "INDEX_PATH = \"fashion_index_v1.pkl\"\n",
    "DATA_FOLDER = '../datasets' \n",
    "\n",
    "class MultiTaskResNet(nn.Module):\n",
    "    def __init__(self, num_categories=50, num_attributes=25):\n",
    "        super(MultiTaskResNet, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=None) \n",
    "        n_inputs = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.category_head = nn.Linear(n_inputs, num_categories)\n",
    "        self.attribute_head = nn.Linear(n_inputs, num_attributes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        cat_output = self.category_head(features)\n",
    "        attr_output = self.attribute_head(features)\n",
    "        return cat_output, attr_output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiTaskResNet()\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loading Search Index...\")\n",
    "with open(INDEX_PATH, \"rb\") as f:\n",
    "    index_data = pickle.load(f)\n",
    "\n",
    "search_engine = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
    "search_engine.fit(index_data[\"embeddings\"])\n",
    "print(\"Search Engine Ready!\")\n",
    "\n",
    "def search_similar_images(query_image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    try:\n",
    "        image = Image.open(query_image_path).convert('RGB')\n",
    "    except:\n",
    "        print(f\"Error: Could not open {query_image_path}\")\n",
    "        return\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        query_vector = model.backbone(img_tensor).cpu().numpy()\n",
    "    distances, indices = search_engine.kneighbors(query_vector)\n",
    "    fig, axes = plt.subplots(1, 6, figsize=(20, 5))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Your Query\")\n",
    "    axes[0].axis(\"off\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        match_path = index_data[\"paths\"][idx]\n",
    "        full_path = os.path.join(DATA_FOLDER, match_path)\n",
    "        try:\n",
    "            match_img = Image.open(full_path)\n",
    "            axes[i+1].imshow(match_img)\n",
    "            axes[i+1].set_title(f\"Match {i+1}\\nDist: {distances[0][i]:.3f}\")\n",
    "            axes[i+1].axis(\"off\")\n",
    "        except:\n",
    "            print(f\"Could not load match: {full_path}\")\n",
    "    plt.show()\n",
    "\n",
    "test_image = os.path.join(DATA_FOLDER, index_data[\"paths\"][110]) \n",
    "search_similar_images(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = os.path.join(DATA_FOLDER, index_data[\"paths\"][6]) \n",
    "search_similar_images(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = os.path.join(DATA_FOLDER, index_data[\"paths\"][64]) \n",
    "search_similar_images(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = os.path.join(DATA_FOLDER, index_data[\"paths\"][43644]) \n",
    "search_similar_images(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = os.path.join(DATA_FOLDER, index_data[\"paths\"][93644]) \n",
    "search_similar_images(test_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
